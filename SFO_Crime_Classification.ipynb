{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj:                   To classify San Francisco Crime Description into one of 33 pre-defined categories.\n",
    "# Data Source (Kaggle) : https://www.kaggle.com/c/sf-crime/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "# Building the Spark Session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"SFO_Crime_Classif\") \\\n",
    "        .config('spark.some.config.option','some-value') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# here, 'spark' is an object of SparkSession, which has the 'SparkContext' object and can be accessed directly\n",
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the csv files:\n",
    "\n",
    "train_data = spark.read.format('com.databricks.spark.csv') \\\n",
    "        .options(header='True', inferschema='True') \\\n",
    "        .load('/home/ramscrux7757/SPARK/SFO_Crime_Class/train.csv')\n",
    "\n",
    "test_data = spark.read.format('com.databricks.spark.csv') \\\n",
    "       .options(header='True', inferschema='True') \\\n",
    "       .load('/home/ramscrux7757/SPARK/SFO_Crime_Class/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 9)\n",
      "(884262, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.count(), len(train_data.columns))\n",
    "print(test_data.count(), len(test_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are huge in size and will be using a sampled datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
      "['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>PARK</td>\n",
       "      <td>NONE</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                      Descript  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "3 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "4 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "\n",
       "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
       "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
       "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
       "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.425892  37.774599  \n",
       "1 -122.425892  37.774599  \n",
       "2 -122.424363  37.800414  \n",
       "3 -122.426995  37.800873  \n",
       "4 -122.438738  37.771541  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|            Descript|\n",
      "+--------------+--------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# since the task is to catogorize the cirme/incident based on the description, \n",
    "# we are dropping all other irrelavant data except Category and Description \n",
    "\n",
    "train_cln = train_data.select(['Category','Descript'])\n",
    "#cols_to_drop = ['Dates','DayOfWeek','PdDistrict','Resolution','Address','X','Y']\n",
    "#train_cln = train_data.select([col for col in train_data.columns if col not in cols_to_drop])\n",
    "train_cln.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the data types are correctly identified or not\n",
    "train_cln.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the distinct categories in 'Category'\n",
    "# Registering the table as a View (for sql kind of exploration)\n",
    "train_cln.createOrReplaceTempView('View')\n",
    "df = spark.sql('select Distinct(Category) from View')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 39 distinct categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            Category| count|\n",
      "+--------------------+------+\n",
      "|       LARCENY/THEFT|174900|\n",
      "|      OTHER OFFENSES|126182|\n",
      "|        NON-CRIMINAL| 92304|\n",
      "|             ASSAULT| 76876|\n",
      "|       DRUG/NARCOTIC| 53971|\n",
      "|       VEHICLE THEFT| 53781|\n",
      "|           VANDALISM| 44725|\n",
      "|            WARRANTS| 42214|\n",
      "|            BURGLARY| 36755|\n",
      "|      SUSPICIOUS OCC| 31414|\n",
      "|      MISSING PERSON| 25989|\n",
      "|             ROBBERY| 23000|\n",
      "|               FRAUD| 16679|\n",
      "|FORGERY/COUNTERFE...| 10609|\n",
      "|     SECONDARY CODES|  9985|\n",
      "|         WEAPON LAWS|  8555|\n",
      "|        PROSTITUTION|  7484|\n",
      "|            TRESPASS|  7326|\n",
      "|     STOLEN PROPERTY|  4540|\n",
      "|SEX OFFENSES FORC...|  4388|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_cln.groupBy('Category') \\\n",
    "         .count() \\\n",
    "         .orderBy(col('count').desc()) \\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Descript|count|\n",
      "+--------------------+-----+\n",
      "|GRAND THEFT FROM ...|60022|\n",
      "|       LOST PROPERTY|31729|\n",
      "|             BATTERY|27441|\n",
      "|   STOLEN AUTOMOBILE|26897|\n",
      "|DRIVERS LICENSE, ...|26839|\n",
      "|      WARRANT ARREST|23754|\n",
      "|SUSPICIOUS OCCURR...|21891|\n",
      "|AIDED CASE, MENTA...|21497|\n",
      "|PETTY THEFT FROM ...|19771|\n",
      "|MALICIOUS MISCHIE...|17789|\n",
      "|   TRAFFIC VIOLATION|16471|\n",
      "|PETTY THEFT OF PR...|16196|\n",
      "|MALICIOUS MISCHIE...|15957|\n",
      "|THREATS AGAINST LIFE|14716|\n",
      "|      FOUND PROPERTY|12146|\n",
      "|ENROUTE TO OUTSID...|11470|\n",
      "|GRAND THEFT OF PR...|11010|\n",
      "|POSSESSION OF NAR...|10050|\n",
      "|PETTY THEFT FROM ...|10029|\n",
      "|PETTY THEFT SHOPL...| 9571|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top crime descriptions (grouping based on 'Descript')\n",
    "train_cln.groupBy('Descript') \\\n",
    "         .count() \\\n",
    "         .orderBy(col('count').desc()) \\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878049\n",
      "8639\n"
     ]
    }
   ],
   "source": [
    "# Subsampling the data (for the sake of running on this machine)\n",
    "print(train_cln.count())\n",
    "train_samp = train_cln.sample(False,0.01, 101)\n",
    "print(train_samp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>PETTY THEFT FROM A BUILDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>ENROUTE TO PAROLE OFFICER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT SHOPLIFTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FRAUD</td>\n",
       "      <td>CREDIT CARD, THEFT BY USE OF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TAMPERING WITH A VEHICLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Category                      Descript\n",
       "0   LARCENY/THEFT   PETTY THEFT FROM A BUILDING\n",
       "1        WARRANTS     ENROUTE TO PAROLE OFFICER\n",
       "2   LARCENY/THEFT       GRAND THEFT SHOPLIFTING\n",
       "3           FRAUD  CREDIT CARD, THEFT BY USE OF\n",
       "4  OTHER OFFENSES      TAMPERING WITH A VEHICLE"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset used for the modeling purposes\n",
    "train_samp.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model pipeline includes the following steps\n",
    "# 1. regexTokenizer (Tokenization with Regular Expressions)\n",
    "# 2. stopwordsRemover\n",
    "# 3. countVectors (Count vectors ('document-term vectors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,StopWordsRemover,CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# regular expression tokenizer \n",
    "regexTokenizer = RegexTokenizer(inputCol = 'Descript', outputCol='words', pattern='\\\\W')\n",
    "# splits the sentence into words\n",
    "\n",
    "# stop words\n",
    "add_stopwords = ['http','https','amp','rt','t','c','the']\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol='words', outputCol='filtered').setStopWords(add_stopwords)\n",
    "\n",
    "#doc is converted into vector of tokens and counts - this is one way of vectorizing\n",
    "countVectors = CountVectorizer(inputCol='filtered',outputCol = 'features', vocabSize=10000, minDF=5)\n",
    "\n",
    "# TF-IDF way of vectorizing (in spark, it is implemented as HashingTF) (statistically popularized vectorization)\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| LARCENY/THEFT|PETTY THEFT FROM ...|[petty, theft, fr...|[petty, theft, fr...|(322,[0,2,7,9,41]...|  0.0|\n",
      "|      WARRANTS|ENROUTE TO PAROLE...|[enroute, to, par...|[enroute, to, par...|(322,[26,42,102,1...|  7.0|\n",
      "| LARCENY/THEFT|GRAND THEFT SHOPL...|[grand, theft, sh...|[grand, theft, sh...|(322,[0,3,72],[1....|  0.0|\n",
      "|         FRAUD|CREDIT CARD, THEF...|[credit, card, th...|[credit, card, th...|(322,[0,1,52,62,6...| 12.0|\n",
      "|OTHER OFFENSES|TAMPERING WITH A ...|[tampering, with,...|[tampering, with,...|(322,[9,18,25,228...|  1.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sting Indexing (the most frequent one gets '0')\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer,VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = 'Category', outputCol='label')\n",
    "# This is the output\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "#pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents\n",
    "pipelineFit = pipeline.fit(train_samp)\n",
    "train_prepr = pipelineFit.transform(train_samp)\n",
    "train_prepr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|            filtered|         rawFeatures|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| LARCENY/THEFT|PETTY THEFT FROM ...|[petty, theft, fr...|[petty, theft, fr...|(10000,[274,3170,...|(10000,[274,3170,...|  0.0|\n",
      "|      WARRANTS|ENROUTE TO PAROLE...|[enroute, to, par...|[enroute, to, par...|(10000,[1245,3258...|(10000,[1245,3258...|  7.0|\n",
      "| LARCENY/THEFT|GRAND THEFT SHOPL...|[grand, theft, sh...|[grand, theft, sh...|(10000,[274,713,9...|(10000,[274,713,9...|  0.0|\n",
      "|         FRAUD|CREDIT CARD, THEF...|[credit, card, th...|[credit, card, th...|(10000,[274,2144,...|(10000,[274,2144,...| 12.0|\n",
      "|OTHER OFFENSES|TAMPERING WITH A ...|[tampering, with,...|[tampering, with,...|(10000,[1158,2895...|(10000,[1158,2895...|  1.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sting Indexing (the most frequent one gets '0')\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents\n",
    "pipelineFit = pipeline.fit(train_samp)\n",
    "train_prepr = pipelineFit.transform(train_samp)\n",
    "train_prepr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Though, I showed both the vecotrizations, in the following I use TF-IDF way of vectorized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count:6051\n",
      "Test Dataset Count:2588\n"
     ]
    }
   ],
   "source": [
    "# Partition the data into train and tests\n",
    "(train, test) = train_prepr.randomSplit([0.7,0.3], seed=101)\n",
    "print('Training Dataset Count:' + str(train.count()))\n",
    "print('Test Dataset Count:'+ str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "|            Descript|     Category|         probability|label|prediction|\n",
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.99999999582460...|  0.0|       0.0|\n",
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "\n",
    "#predictions.head(3)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9670678743353475"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "|            Descript|     Category|         probability|label|prediction|\n",
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM ...|LARCENY/THEFT|[0.72457857067092...|  0.0|       0.0|\n",
      "+--------------------+-------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(test)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6838730954522798"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create ParamGrid for Cross Validation\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 200]) # number of trees\n",
    "             .addGrid(rf.maxDepth, [3, 4, 5]) # maximum depth\n",
    "#            .addGrid(rf.maxBins, [24, 32, 40]) #Number of bins\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7412958347068506"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
