{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString,VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "# Building the Spark Session\n",
    "# Befire 2.0.0, the main connection objects were 'SaprkContext,SqlContext, and HiveContext'\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Movie_Recommendations\") \\\n",
    "        .config('spark.some.config.option','some-value') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# here, 'spark' is an object of SparkSession, which has the 'SparkContext' object and can be accessed directly\n",
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "hive.metastore.warehouse.dir=file:/home/ramscrux7757/spark-warehouse\n",
      "spark.app.id=local-1521775083021\n",
      "spark.app.name=Movie_Recommendations\n",
      "spark.driver.host=172.31.60.179\n",
      "spark.driver.port=50551\n",
      "spark.executor.id=driver\n",
      "spark.files=file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar,file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-csv_2.11-1.5.0.jar,file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-avro_2.11-3.2.0.jar,file:/home/ramscrux7757/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ramscrux7757/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:/home/ramscrux7757/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:/home/ramscrux7757/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:/home/ramscrux7757/.ivy2/jars/org.tukaani_xz-1.0.jar,file:/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/home/ramscrux7757/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/home/ramscrux7757/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar\n",
      "spark.jars=file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar,file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-csv_2.11-1.5.0.jar,file:/home/ramscrux7757/.ivy2/jars/com.databricks_spark-avro_2.11-3.2.0.jar,file:/home/ramscrux7757/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,file:/home/ramscrux7757/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,file:/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,file:/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,file:/home/ramscrux7757/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,file:/home/ramscrux7757/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,file:/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,file:/home/ramscrux7757/.ivy2/jars/org.tukaani_xz-1.0.jar,file:/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/home/ramscrux7757/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/home/ramscrux7757/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar\n",
      "spark.jars.packages=com.databricks:spark-xml_2.11:0.4.1,com.databricks:spark-csv_2.11:1.5.0,com.databricks:spark-avro_2.11:3.2.0,graphframes:graphframes:0.5.0-spark2.0-s_2.11\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.some.config.option=some-value\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=/home/ramscrux7757/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar,/home/ramscrux7757/.ivy2/jars/com.databricks_spark-csv_2.11-1.5.0.jar,/home/ramscrux7757/.ivy2/jars/com.databricks_spark-avro_2.11-3.2.0.jar,/home/ramscrux7757/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar,/home/ramscrux7757/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar,/home/ramscrux7757/.ivy2/jars/org.apache.avro_avro-1.7.6.jar,/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar,/home/ramscrux7757/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar,/home/ramscrux7757/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar,/home/ramscrux7757/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar,/home/ramscrux7757/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar,/home/ramscrux7757/.ivy2/jars/org.tukaani_xz-1.0.jar,/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,/home/ramscrux7757/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,/home/ramscrux7757/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,/home/ramscrux7757/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar\n"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism) # number of default number of partitions\n",
    "print (sc.getConf().toDebugString()) # details about the spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9125, 3)\n",
      "(100004, 4)\n"
     ]
    }
   ],
   "source": [
    "movies_data = spark.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .option('header','true') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load('/home/ramscrux7757/SPARK/ml-latest-small/movies.csv')\n",
    "    \n",
    "ratings_data = spark.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .option('header','true') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .load('/home/ramscrux7757/SPARK/ml-latest-small/ratings.csv')\n",
    "\n",
    "print(movies_data.count(), len(movies_data.columns))\n",
    "print(ratings_data.count(), len(ratings_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(movies_data.rdd.getNumPartitions())\n",
    "print(ratings_data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# despite the default parallelism, the data has been placed within 1 executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Repartitioning the data\n",
    "# here, the data has been distributed across '4' executors\n",
    "repart_movies = movies_data.repartition(4)\n",
    "repart_ratings = ratings_data.repartition(4)\n",
    "print(repart_movies.rdd.getNumPartitions())\n",
    "print(repart_ratings.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#movies_data.cache()\n",
    "print(movies_data.printSchema())\n",
    "print(ratings_data.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1029</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1260759179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1263</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1343</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1260759131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2105</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260759139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1     1029     3.0  1260759179\n",
       "1       1     1263     2.0  1260759151\n",
       "2       1     1343     2.0  1260759131\n",
       "3       1     2105     4.0  1260759139"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# easy to switch between spark (distributed or single) to native Pandas DF\n",
    "#ratings_data.toPandas().head()\n",
    "pd.DataFrame(repart_ratings.take(4), columns = repart_ratings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache - both the repartitioned movies and ratings dataframes \n",
    "repart_movies.cache()\n",
    "repart_ratings.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|summary|           userId|           movieId|            rating|           timestamp|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|  count|           100004|            100004|            100004|              100004|\n",
      "|   mean|347.0113095476181|12548.664363425463| 3.543608255669773|1.1296390869392424E9|\n",
      "| stddev|195.1638379781962|26369.198968815206|1.0580641091070389|1.9168582602710965E8|\n",
      "|    min|                1|                 1|               0.5|           789652009|\n",
      "|    max|              671|            163949|               5.0|          1476640644|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repart_ratings.describe().show()\n",
    "# usually, not all these are meaninngful and useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Users:', 671)\n",
      "('Total movies:', 9066)\n",
      "('Total most popular Movies:', 4035)\n",
      "('Count of different genres:', 902)\n"
     ]
    }
   ],
   "source": [
    "# unique user ID's \n",
    "print('Total Users:', repart_ratings.select(repart_ratings.userId).distinct().count())\n",
    "# unique movie ID's\n",
    "print('Total movies:', repart_ratings.select(repart_ratings.movieId).distinct().count())\n",
    "# number of movies with at least one rating higher than 4\n",
    "print('Total most popular Movies:', repart_ratings.filter(repart_ratings.rating > 4).select(repart_ratings.movieId).distinct().count())\n",
    "# number of different genres\n",
    "print('Count of different genres:', repart_movies.select(repart_movies.genres).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  nb|\n",
      "+----+\n",
      "|4035|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL way of querying\n",
    "repart_ratings.createOrReplaceTempView('ratings')\n",
    "spark.sql(\"SELECT COUNT(DISTINCT(movieId)) AS nb FROM ratings WHERE rating > 4\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+\n",
      "|movieId|       avg(rating)|               title|              genres|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "|   3175|3.5076923076923077| Galaxy Quest (1999)|Adventure|Comedy|...|\n",
      "|  96488|              3.75|Searching for Sug...|         Documentary|\n",
      "|   1580| 3.663157894736842|Men in Black (a.k...|Action|Comedy|Sci-Fi|\n",
      "|   7982|3.1666666666666665|Tale of Two Siste...|Drama|Horror|Myst...|\n",
      "|   1088| 3.358490566037736|Dirty Dancing (1987)|Drama|Musical|Rom...|\n",
      "|   1238| 4.147058823529412|   Local Hero (1983)|              Comedy|\n",
      "|   1342|3.0588235294117645|     Candyman (1992)|     Horror|Thriller|\n",
      "|   6620|3.6470588235294117|American Splendor...|        Comedy|Drama|\n",
      "|   1645|3.4583333333333335|The Devil's Advoc...|Drama|Mystery|Thr...|\n",
      "|   3794|               3.4| Chuck & Buck (2000)|        Comedy|Drama|\n",
      "|   1959|               3.8|Out of Africa (1985)|       Drama|Romance|\n",
      "|   2122|2.3181818181818183|Children of the C...|     Horror|Thriller|\n",
      "|   1591|               2.7|        Spawn (1997)|Action|Adventure|...|\n",
      "|   8638|3.8823529411764706|Before Sunset (2004)|       Drama|Romance|\n",
      "|    471| 3.877551020408163|Hudsucker Proxy, ...|              Comedy|\n",
      "|  44022|             3.175|Ice Age 2: The Me...|Adventure|Animati...|\n",
      "|   2142|              3.25|American Tail: Fi...|Adventure|Animati...|\n",
      "|   4519|               3.4|Land Before Time,...|Adventure|Animati...|\n",
      "|  54190|               3.4|Across the Univer...|Drama|Fantasy|Mus...|\n",
      "|  68135|3.6818181818181817|     17 Again (2009)|        Comedy|Drama|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average rating of each movie  -- this lets us to pick the highly rated movies aswell !!!\n",
    "repart_ratings.groupby('movieId').agg({'rating':'mean'}) \\\n",
    "                .join(repart_movies, repart_ratings.movieId == repart_movies.movieId, 'inner') \\\n",
    "                .drop(repart_movies.movieId) \\\n",
    "                .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     0|      0|     0|        0|\n",
      "+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for the missing values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "repart_ratings.select([count(when(isnan(c),c)).alias(c) for c in repart_ratings.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|movieId|title|genres|\n",
      "+-------+-----+------+\n",
      "|      0|    0|     0|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repart_movies.select([count(when(isnan(c),c)).alias(c) for c in repart_movies.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identifying the linar relation between usedId's and movieId's\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#ratingsPandas = repart_ratings.toPandas()\n",
    "#lm = sns.lmplot(x='userId', y='movieId', data=ratingsPandas, fit_reg=False, size=10, \n",
    "#                aspect=2, palette=sns.diverging_palette(10, 133, sep=80, n=10))\n",
    "\n",
    "#axes = lm.axes\n",
    "#axes[0,0].set_ylim(0,163950)\n",
    "#axes[0,0].set_xlim(0,675)\n",
    "#lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+-----------+------------------+\n",
      "|nb_users|nb_movies|nb_ratings|matrix_size|        percentage|\n",
      "+--------+---------+----------+-----------+------------------+\n",
      "|     671|     9066|    100004|    6083286|1.6439141608663477|\n",
      "+--------+---------+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building the recommender system\n",
    "# 3 types of algorithms: user-based, content-based and collaborative filtering\n",
    "# here, we are using 'collaborative filtering' approach\n",
    "# alternating least squares (ALS) algorithm provides collaborative filetering between users and products (moves here)\n",
    "\n",
    "# chekcing the sparcity of the user/product(movies) matrix\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, 100 * nb_ratings/matrix_size AS percentage\n",
    "    FROM (\n",
    "        SELECT nb_users, nb_movies, nb_ratings, nb_users * nb_movies AS matrix_size\n",
    "        FROM (\n",
    "            SELECT COUNT(*) AS nb_ratings, COUNT(DISTINCT(movieId)) AS nb_movies, COUNT(DISTINCT(userId)) AS nb_users\n",
    "            FROM ratings\n",
    "        )\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# less than 2% of the matrix is filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "(trainingRatings, testRatings) = repart_ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "model = als.fit(trainingRatings)\n",
    "predictions = model.transform(testRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|   380|    463|   3.0| 968949106| 2.9540255|\n",
      "|   602|    471|   3.0| 842357922| 4.0983095|\n",
      "|   274|    471|   5.0|1074104142|  3.724506|\n",
      "|   440|    471|   3.0| 835337519| 3.4512959|\n",
      "|   292|    471|   3.5|1140049920| 3.9857833|\n",
      "|   452|    471|   3.0| 976422396| 3.3047695|\n",
      "|    19|    471|   3.0| 855192558|  3.776581|\n",
      "|   309|    471|   4.0|1114565458| 4.0564013|\n",
      "|    15|    471|   3.0|1166586067|  2.988698|\n",
      "|   659|    471|   4.0| 853412972| 3.5642576|\n",
      "|   102|    471|   5.0| 958248997| 4.3238254|\n",
      "|    73|    471|   4.0|1296460183| 3.8675725|\n",
      "|   508|    471|   4.0| 844377075|  4.092397|\n",
      "|   242|    471|   5.0| 956686752| 4.5214157|\n",
      "|   468|    471|   4.0|1296197444| 3.4027889|\n",
      "|   497|    496|   2.0| 939767844| 2.7011259|\n",
      "|   294|    833|   2.0|1047074195| 2.5256567|\n",
      "|   128|   1088|   5.0|1049690378| 4.9113264|\n",
      "|   111|   1088|   3.5|1097431651| 3.4089887|\n",
      "|   500|   1088|   4.0|1229098924| 3.4455402|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|userId|movieId|rating|timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "|     0|      0|     0|        0|       708|\n",
      "+------+-------+------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select([count(when(isnan(c),c)).alias(c) for c in predictions.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating in the dataset is: 3.54360825567\n",
      "('By dropping the NaN records:', 0.9166879407205767)\n",
      "(\"By replacing the NaN's with avg:\", 0.9284420923478783)\n"
     ]
    }
   ],
   "source": [
    "# replacing the Nan's in prediction columns with 'average ratings'\n",
    "\n",
    "avgRatings = repart_ratings.select('rating').groupBy().avg().first()[0]\n",
    "print \"The average rating in the dataset is: \" + str(avgRatings)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "print('By dropping the NaN records:', evaluator.evaluate(predictions.na.drop()))\n",
    "print('By replacing the NaN\\'s with avg:', evaluator.evaluate(predictions.na.fill(avgRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-Fold cross-validation\n",
    "\n",
    "#def kfoldALS(data, k=3, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", metricName=\"rmse\"):\n",
    "#    evaluations = []\n",
    "#    weights = [1.0] * k\n",
    "#    splits = data.randomSplit(weights)\n",
    "#    for i in range(0, k):  \n",
    "#        testingSet = splits[i]\n",
    "#        trainingSet = spark.createDataFrame(sc.emptyRDD(), data.schema)\n",
    "#        for j in range(0, k):\n",
    "#            if i == j:\n",
    "#                continue\n",
    "#            else:\n",
    "#                trainingSet = trainingSet.union(splits[j])\n",
    "#        als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol)\n",
    "#        model = als.fit(trainingSet)\n",
    "#        predictions = model.transform(testingSet)\n",
    "#        evaluator = RegressionEvaluator(metricName=metricName, labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "#        evaluation = evaluator.evaluate(predictions.na.drop())\n",
    "#        print \"Loop \" + str(i+1) + \": \" + metricName + \" = \" + str(evaluation)\n",
    "#        evaluations.append(evaluation)\n",
    "#    return sum(evaluations)/float(len(evaluations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('RMSE = ', kfoldALS(repart_ratings, k=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: False)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: max number of iterations (>= 0). (default: 10)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: False)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0). (default: 0.1)\n",
      "seed: random seed. (default: 593367982098446717)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "print(als.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root mean squared error for cross-validated model is: 0.980104521746\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#(trainingRatings, validationRatings) = repart_ratings.randomSplit([0.8, 0.2])\n",
    "#evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [1, 5, 10]) \\\n",
    "        .addGrid(als.maxIter, [10]) \\\n",
    "        .addGrid(als.regParam, [0.05, 0.1, 0.5]) \\\n",
    "        .build()\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(trainingRatings)\n",
    "predictions = cvModel.transform(testRatings)\n",
    "\n",
    "print \"The root mean squared error for cross-validated model is: \" + str(evaluator.evaluate(predictions.na.fill(avgRatings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommended movies for the new users\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def recommendMovies(model, user, nbRecommendations):\n",
    "    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n",
    "    dataSet = repart_ratings.select(\"movieId\").distinct().withColumn(\"userId\", lit(user))\n",
    "\n",
    "    # Create a Spark DataFrame with the movies that have already been rated by this user\n",
    "    moviesAlreadyRated = repart_ratings.filter(repart_ratings.userId == user).select(\"movieId\", \"userId\")\n",
    "\n",
    "    # Apply the recommender system to the data set without the already rated movies to predict ratings\n",
    "    predictions = model.transform(dataSet.subtract(moviesAlreadyRated)) \\\n",
    "                        .dropna().orderBy(\"prediction\", ascending=False) \\\n",
    "                        .limit(nbRecommendations).select(\"movieId\", \"prediction\")\n",
    "\n",
    "    # Join with the movies DataFrame to get the movies titles and genres\n",
    "    recommendations = predictions.join(repart_movies, predictions.movieId == repart_movies.movieId) \\\n",
    "                                .select(predictions.movieId, repart_movies.title, repart_movies.genres, \\\n",
    "                                        predictions.prediction)\n",
    "\n",
    "    recommendations.show(truncate=False)\n",
    "    #print(dataSet.show())\n",
    "    #print(moviesAlreadyRated.show())\n",
    "    #print(predictions.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 133:\n",
      "+-------+--------------------------------------+-----------------------------+----------+\n",
      "|movieId|title                                 |genres                       |prediction|\n",
      "+-------+--------------------------------------+-----------------------------+----------+\n",
      "|4930   |Funeral in Berlin (1966)              |Action|Drama|Thriller        |6.7820787 |\n",
      "|4796   |Grass Is Greener, The (1960)          |Comedy|Romance               |6.7820787 |\n",
      "|4591   |Erik the Viking (1989)                |Adventure|Comedy|Fantasy     |6.7820787 |\n",
      "|3892   |Anatomy (Anatomie) (2000)             |Horror                       |6.7820787 |\n",
      "|1563   |Dream With the Fishes (1997)          |Drama                        |6.7820787 |\n",
      "|1819   |Storefront Hitchcock (1997)           |Documentary|Musical          |6.7820787 |\n",
      "|3216   |Vampyros Lesbos (Vampiras, Las) (1971)|Fantasy|Horror|Thriller      |4.049766  |\n",
      "|97957  |Excision (2012)                       |Crime|Drama|Horror|Thriller  |4.049766  |\n",
      "|4754   |Wicker Man, The (1973)                |Drama|Horror|Mystery|Thriller|4.049766  |\n",
      "|5071   |Maelstr√∂m (2000)                      |Drama|Romance                |3.9077504 |\n",
      "+-------+--------------------------------------+-----------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Recommendations for user 133:\"\n",
    "recommendMovies(cvModel, 133, 10)\n",
    "#print \"Recommendations for user 471:\"\n",
    "#recommendMovies(cvModel, 471, 10)\n",
    "#print \"Recommendations for user 496:\"\n",
    "#recommendMovies(cvModel, 496, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
